========================================
1.   페이지 43 (1월 5일)
========================================

주제: 초기 서버 구조 설계 및 기본 구성

수행 내용:
- SAM-6D 기반 마이크로서비스 아키텍처 설계: Main_Server(8001), ISM_Server(8002), PEM_Server(8003), Render_Server(8004)로 분리。
- Main_Server의 기본 구조 구축: FastAPI 기반 중앙 오케스트레이션 서버, Static 폴더 스캔 로직, 서버 상태 모니터링 API 구현。
- 각 서버별 독립적인 FastAPI 애플리케이션 구조 설계, 환경 변수 기반 설정 시스템 구축 (config.env, config_dev.env 분리)。
- 프로젝트 루트 경로 관리 및 상대 경로 처리 로직 구현, 로깅 시스템 기본 틀 작성。

발생 문제:
- 프로젝트 루트 경로 계산이 여러 실행 경로에서 일관되지 않아 Path.resolve()를 사용한 절대 경로 기반으로 변경。
- 서버 간 통신 시 포트 및 호스트 정보를 하드코딩했던 부분을 환경 변수로 추출하는 리팩토링 필요。

소견:
마이크로서비스로 분리하니 각 서버의 책임이 명확해지고 독립적인 배포와 확장이 가능해졌다. 다만 초기에는 경로 처리와 환경 변수 관리에 시간을 많이 썼다. 이후 각 서버의 독립적인 main.py와 라우터 구조를 확립하면서 개발 속도가 향상되었다。

----------------------------------------

========================================
2.   페이지 44 (1월 12일)
========================================

주제: 각 서버별 핵심 기능 구현

수행 내용:
- ISM_Server 구현: SAM-6D 모델 로딩, Hydra 설정 파일 통합, Base64 이미지 디코딩 및 템플릿 기반 세그멘테이션 추론 API 구현。
- PEM_Server 구현: PointNet2 기반 포즈 추정 모델 통합, ISM 결과를 받아 6D 포즈 계산하는 API 구현, 모델 매니저 및 캐시 시스템 기본 구조 작성。
- Render_Server 구현: BlenderProc 기반 템플릿 렌더링 작업 큐 시스템, 비동기 작업 처리 (job_id 기반), 다중 시점 렌더링 (RGB, Depth, Mask) 지원。
- 각 서버의 헬스 체크 API 및 상태 확인 엔드포인트 구현, 서버 시작 시 자동 모델 로딩 기능 추가。

발생 문제:
- ISM_Server에서 SAM-6D 모듈 import 경로가 복잡해 상대 경로 계산 로직이 여러 번 수정됨。
- PEM_Server의 PointNet2 CUDA 확장 빌드가 Windows 환경에서 실패하여 Docker 환경에서만 동작하도록 제한。
- Render_Server의 BlenderProc 실행이 서브프로세스로 동작하지 않아 async 작업 큐로 전환하는 과정에서 지연 발생。

소견:
각 서버의 핵심 기능을 독립적으로 구현하면서도 전체 파이프라인을 염두에 두고 API 인터페이스를 설계했다. 특히 Base64 이미지 인코딩/디코딩을 표준화하여 서버 간 데이터 전달을 일관되게 유지했다. 모델 로딩 시간이 길어 서버 시작 시 자동 로딩을 추가했는데, 이는 이후 캐싱 시스템으로 발전하게 된다。

----------------------------------------

========================================
3.   페이지 45 (1월 18일)
========================================

주제: 워크플로우 오케스트레이션 구현

수행 내용:
- Main_Server의 WorkflowService 구현: Render → ISM → PEM 순서의 전체 파이프라인 자동 실행 로직 작성。
- 템플릿 존재 여부 확인 후 없으면 Render_Server 호출, 있으면 스킵하는 조건부 실행 로직 구현。
- 각 서버 호출 시 에러 핸들링 및 재시도 로직 추가, 타임스탬프 기반 출력 디렉토리 자동 생성 (`static/output/<timestamp>_<tag>/` 형식)。
- 전체 파이프라인 결과 통합 및 최종 포즈 결과 요약 API 구현 (`/api/v1/workflow/full-pipeline`), 배치 처리 지원 (`/api/v1/workflow/render-templates`)。

발생 문제:
- 서버 간 비동기 호출 시 타임아웃 설정이 불충분해 일부 긴 작업에서 연결이 끊기는 문제 발생, 타임아웃을 900초로 증가하고 재시도 로직 강화。
- 출력 경로가 Windows와 Linux에서 다르게 처리되어 Path 객체를 사용한 플랫폼 독립적인 경로 처리로 변경。
- ISM 결과를 PEM으로 전달할 때 JSON 직렬화/역직렬화 과정에서 데이터 손실이 발생하여 구조화된 데이터 모델로 표준화.

소견:
전체 파이프라인을 자동화하면서 각 단계의 결과를 다음 단계로 전달하는 인터페이스를 명확히 정의했다. 특히 출력 경로와 태그 시스템을 통해 결과 추적과 디버깅이 용이해졌다. 초기에는 동기식으로 구현했다가 async/await로 전환하면서 성능과 안정성이 크게 향상되었다。

----------------------------------------

========================================
4.   페이지 46 (1월 25일)
========================================

주제: RSS 서버 연동 및 Base64 이미지 처리

수행 내용:
- RSS 서버에서 RealSense 카메라 스트림을 직접 수집하는 API 구현 (`/api/v1/workflow/full-pipeline-from-rss`): status, calib, color_jpeg, color_raw, depth_raw 스트림 순차 가져오기。
- Base64 이미지 인코딩/디코딩 최적화: PIL Image와 numpy 배열 변환 로직 개선, 메모리 효율적인 스트림 처리 구현。
- 카메라 파라미터 자동 구성: RSS에서 제공하는 calibration 정보를 cam_K와 depth_scale로 변환하는 로직 작성, depth 정합 옵션 (`align_color`) 지원。
- 출력 모드 선택 기능 추가: `full`(모든 파일 저장), `results_only`(포즈 결과만), `none`(파일 저장 없음) 세 가지 모드 지원, 클라이언트 요구에 따른 유연한 결과 제공。

발생 문제:
- RSS 서버의 스트림 URL 구조가 문서화되지 않아 실제 테스트를 통해 경로를 확인해야 했음 (`/streams/status`, `/streams/color_jpeg` 등)。
- Base64 인코딩된 이미지가 일부 손상되어 전달되는 경우가 있어 이미지 검증 로직 추가 (PIL Image.open() 예외 처리)。
- Depth 이미지와 RGB 이미지의 해상도가 다른 경우를 처리하기 위한 리샘플링 로직 구현에 시간 소요。

소견:
RSS 서버와의 연동을 통해 실시간 카메라 스트림을 직접 활용할 수 있게 되면서 시스템의 실용성이 크게 향상되었다. Base64 처리 과정에서 메모리 효율성을 고려한 최적화를 통해 대용량 이미지도 안정적으로 처리할 수 있게 되었다. 출력 모드 선택 기능은 개발/운영 환경에서 필요한 정보만 선택적으로 저장할 수 있어 디스크 공간 관리에 도움이 되었다.

----------------------------------------

========================================
5.   페이지 47 (1월 31일)
========================================

주제: 캐싱 시스템 및 성능 최적화

수행 내용:
- ISM_Server에 LRU 캐시 시스템 구현: 템플릿 데이터와 CAD 모델 포인트 클라우드를 메모리에 캐싱하여 반복 로딩 시간 단축 (최대 20개 객체, 환경 변수로 조절 가능)。
- PEM_Server에도 동일한 LRU 캐시 적용: 템플릿 캐시와 CAD 캐시를 분리하여 관리, 스레드 안전성을 위한 Lock 객체 사용。
- 서버 시작 시 템플릿 사전 로딩 기능 추가 (`ISM_PRELOAD_TEMPLATES`, `PEM_PRELOAD_TEMPLATES` 환경 변수): static/templates/ycb 경로의 모든 객체를 자동으로 캐시에 로딩。
- 캐시 히트율 모니터링 및 로깅 추가, 캐시 크기 동적 조절 기능 구현 (환경 변수: `ISM_MAX_CACHE_SIZE`, `PEM_TEMPLATE_CACHE_MAX` 등)。

발생 문제:
- LRU 캐시 구현 시 스레드 안전성 문제로 인해 동시 요청에서 캐시 충돌이 발생, threading.Lock을 사용하여 동기화 처리 추가。
- 사전 로딩 시 메모리 사용량이 급증하여 GPU 메모리 부족으로 서버가 다운되는 경우 발생, 캐시 크기 제한을 더 엄격하게 적용하고 선별적 사전 로딩 옵션 추가。
- 캐시 무효화 전략이 부족하여 템플릿 파일이 업데이트되어도 캐시가 갱신되지 않는 문제를 파일 수정 시간 기반 무효화로 해결.

소견:
캐싱 시스템을 도입하면서 동일 객체에 대한 반복 추론 시간이 크게 단축되었다 (템플릿 로딩: ~5초 → ~0.1초). 특히 사전 로딩 기능은 운영 환경에서 자주 사용되는 YCB 객체들을 미리 준비해두어 첫 요청 응답 시간을 크게 개선했다. 다만 메모리 사용량 증가와 캐시 관리 복잡도가 트레이드오프였고, 환경 변수로 세밀하게 조절할 수 있도록 설계했다.

----------------------------------------

========================================
6.   페이지 48 (2월 7일)
========================================

주제: Docker 환경 구성 및 자동 시작 기능

수행 내용:
- 각 서버별 Dockerfile 및 docker-compose.yml 작성: GPU 지원 (NVIDIA Container Toolkit), 볼륨 마운트, 환경 변수 주입 설정。
- Main_Server의 자동 서버 시작 기능 구현: 서버 시작 시 ISM/PEM/Render 서버의 헬스 체크 수행, 실패 시 docker-compose up을 자동 실행 (`AUTO_START_DEPENDENCIES` 환경 변수로 제어)。
- 통합 docker-compose.sam6d.yml 작성: 모든 서버를 한 번에 실행하는 오케스트레이션 파일, 네트워크 및 볼륨 공유 설정。
- Windows/Linux 환경 모두 지원: docker-compose와 docker compose 명령어 모두 지원하도록 subprocess 호출 로직 개선, 경로 처리의 플랫폼 독립성 확보。

발생 문제:
- Windows에서 docker-compose 명령어가 docker compose로 통합되면서 기존 스크립트가 동작하지 않아 두 가지 명령어를 모두 시도하는 폴백 로직 필요。
- Docker 컨테이너 간 네트워크 연결에서 localhost가 아닌 서비스 이름을 사용해야 하는데, 개발 환경에서는 localhost를 사용하는 모순이 발생, 환경 변수로 호스트 주소를 주입받도록 변경。
- GPU 메모리 공유 문제로 여러 서버가 동시에 실행될 때 CUDA out of memory 에러 발생, 각 서버의 모델 로딩 타이밍을 조절하거나 GPU 메모리 제한 설정 추가.

소견:
Docker 환경을 구축하면서 개발/운영 환경의 일관성을 확보했고, 자동 시작 기능으로 서버 관리 부담이 크게 줄어들었다. 특히 Windows와 Linux 모두 지원하면서 다양한 환경에서 테스트할 수 있게 되었다. 다만 Docker 네트워크와 로컬 개발 환경의 차이를 환경 변수로 추상화하는 과정에서 설정 복잡도가 증가했다. 이는 이후 README와 설정 가이드 문서화로 해결 방안을 제공했다.

----------------------------------------

========================================
7.   페이지 49 (2월 14일)
========================================

주제: 테스트 및 모니터링 시스템 구축

수행 내용:
- 서버 상태 모니터링 API 강화: `/api/v1/servers/status`에서 각 서버의 응답 시간, 모델 로딩 상태, GPU 메모리 사용량을 실시간으로 확인 가능하도록 확장。
- 통합 테스트 스크립트 작성: `test_api_full_pipeline.py` (전체 파이프라인), `test_rss_api_call.py` (RSS 연동), `test_ycb_full_pipeline.py` (YCB 객체 배치 처리) 등。
- 로깅 시스템 개선: 날짜별 로그 파일 생성 (`main_server_YYYYMMDD.log`), 각 서버별 독립적인 로그 디렉토리, 로그 레벨 환경 변수 제어 (`LOG_LEVEL`)。
- 배치 처리 결과 요약 기능: `run_rss_batch_ycb.py` 스크립트로 다수 객체를 연속 처리하고 성공/실패 통계를 JSON 파일로 저장 (`rss_batch_summary.json`)。

발생 문제:
- 서버 상태 확인 시 타임아웃이 발생하면 전체 응답이 지연되는 문제를 각 서버별 비동기 병렬 체크로 변경하여 응답 시간 단축。
- 로그 파일이 계속 쌓여 디스크 공간을 과도하게 사용하는 문제가 발생, 로그 로테이션 정책 수립 (30일 이상된 로그 자동 삭제 또는 압축) 필요성 확인。
- 배치 처리 중 일부 객체 실패 시 전체 프로세스가 중단되지 않도록 예외 처리 강화, 부분 실패 시에도 진행 상황을 로그로 기록하도록 개선。

소견:
테스트와 모니터링 시스템을 구축하면서 운영 환경에서의 안정성과 디버깅 용이성이 크게 향상되었다. 특히 배치 처리 결과 요약 기능은 대량의 객체를 처리할 때 전체 진행 상황을 한눈에 파악할 수 있어 유용했다. 로그 시스템은 각 서버별로 독립적으로 관리하면서도 일관된 형식을 유지하도록 설계했는데, 추후 로그 수집 시스템 (Loki/Promtail)과 연동할 수 있는 구조로 확장 가능성을 남겨두었다. 전체 시스템이 프로덕션 환경에 투입 가능한 수준으로 안정화되었다.

----------------------------------------

