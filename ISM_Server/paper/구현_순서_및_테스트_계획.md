# ISM_Server 구현 순서 및 테스트 계획

## 개요
정말 최소 단위의 ISM 서버를 단계별로 구현하고, 각 단계마다 테스트하면서 버그를 잡아나가는 방식으로 진행합니다.

## 구현 순서

### Phase 0: Docker 환경 및 라이브러리 Import 테스트
**목표**: Docker 환경에서 필요한 라이브러리들이 정상적으로 import되는지 확인

#### 0.1 Docker 환경 접속 및 기본 확인
```bash
# Docker 컨테이너 실행
docker-compose -f docker-compose.sam6d.yml up -d sam6d-server

# 컨테이너 접속
docker exec -it sam6d-server bash

# 작업 디렉토리 확인
pwd
ls -la

# Python 환경 확인
conda activate sam6d
python --version
```

#### 0.2 기본 라이브러리 Import 테스트
```bash
# 컨테이너 내부에서 Python 실행
python

# 기본 라이브러리 테스트
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

import numpy as np
print(f"NumPy version: {np.__version__}")

from PIL import Image
print("PIL imported successfully")

import trimesh
print("Trimesh imported successfully")

import cv2
print(f"OpenCV version: {cv2.__version__}")
```

#### 0.3 SAM-6D 관련 라이브러리 Import 테스트
```bash
# SAM-6D 디렉토리로 이동
cd /workspace/Estimation_Server/SAM-6D/SAM-6D/Instance_Segmentation_Model

# Python path 설정
export PYTHONPATH="/workspace/Estimation_Server/SAM-6D/SAM-6D/Instance_Segmentation_Model:$PYTHONPATH"

# SAM-6D 관련 라이브러리 테스트
python -c "
import sys
sys.path.append('/workspace/Estimation_Server/SAM-6D/SAM-6D/Instance_Segmentation_Model')

# Hydra 테스트
from hydra import initialize, compose
print('Hydra imported successfully')

# SAM-6D 모델 테스트
from model.utils import Detections
print('SAM-6D model utils imported successfully')

# 추론 함수 테스트
from run_inference_custom_function import load_templates_from_files
print('run_inference_custom_function imported successfully')
"
```

#### 0.4 FastAPI 라이브러리 설치 및 테스트
```bash
# FastAPI 관련 라이브러리 설치
pip install fastapi uvicorn[standard] pydantic python-multipart

# 설치 확인
python -c "
import fastapi
print(f'FastAPI version: {fastapi.__version__}')

import uvicorn
print('Uvicorn imported successfully')

import pydantic
print(f'Pydantic version: {pydantic.__version__}')
"
```

#### 0.5 성공 기준
- ✅ Docker 컨테이너가 정상적으로 실행됨
- ✅ Python 환경이 정상적으로 설정됨
- ✅ 모든 기본 라이브러리가 정상적으로 import됨
- ✅ SAM-6D 관련 라이브러리가 정상적으로 import됨
- ✅ FastAPI 라이브러리가 정상적으로 설치되고 import됨

---

### Phase 1: 기본 FastAPI 서버 구현 및 테스트
**목표**: 가장 기본적인 FastAPI 서버가 동작하는지 확인

#### 1.1 Docker 컨테이너 내부에서 기본 파일 생성
```bash
# 컨테이너 내부에서 ISM_Server 디렉토리 생성
mkdir -p /workspace/Estimation_Server/ISM_Server
cd /workspace/Estimation_Server/ISM_Server

# 기본 파일 생성
touch main.py
touch requirements.txt
```

#### 1.2 최소 FastAPI 서버 구현
```python
# main.py
from fastapi import FastAPI

app = FastAPI(title="ISM Server", version="1.0.0")

@app.get("/")
async def root():
    return {"message": "ISM Server is running"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "message": "Server is running"}
```

#### 1.3 requirements.txt 생성
```txt
fastapi
uvicorn[standard]
```

#### 1.4 Docker 컨테이너 내부에서 테스트
```bash
# 컨테이너 내부에서 서버 실행
cd /workspace/Estimation_Server/ISM_Server
uvicorn main:app --host 0.0.0.0 --port 8002

# 다른 터미널에서 컨테이너 접속하여 테스트
docker exec -it sam6d-server bash
curl http://localhost:8002/
curl http://localhost:8002/health
```

#### 1.5 성공 기준
- ✅ 서버가 정상적으로 시작됨
- ✅ `/` 엔드포인트가 응답함
- ✅ `/health` 엔드포인트가 응답함
- ✅ 에러 없이 서버가 계속 실행됨

---

### Phase 2: 모델 로딩 기능 구현 및 테스트
**목표**: 기존 SAM-6D 모델과 템플릿을 로딩하는 기능 구현

#### 2.1 필요한 import 추가
```python
# main.py에 추가
import os
import sys
import torch
import trimesh
from PIL import Image
import numpy as np
from hydra import initialize, compose
from hydra.utils import instantiate
from omegaconf import OmegaConf

# 기존 추론 함수 임포트
sys.path.append('/workspace/Estimation_Server/SAM-6D/SAM-6D/Instance_Segmentation_Model')
from run_inference_custom_function import load_templates_from_files
```

#### 2.2 전역 변수 및 모델 로딩 함수 추가
```python
# 전역 변수
model = None
templates_data = None
templates_masks = None
templates_boxes = None
cad_points = None
device = None

async def load_model():
    """모델 로딩 함수"""
    global model, device
    
    try:
        # 환경 변수에서 경로 가져오기
        config_dir = os.getenv('ISM_CONFIG_DIR', '/workspace/Estimation_Server/SAM-6D/SAM-6D/Instance_Segmentation_Model/configs')
        
        # 모델 초기화 (기존 코드 활용)
        with initialize(version_base=None, config_path=config_dir):
            cfg = compose(config_name='run_inference.yaml')
        
        # SAM 모델 설정
        with initialize(version_base=None, config_path=f"{config_dir}/model"):
            cfg.model = compose(config_name='ISM_sam.yaml')
        
        # 모델 인스턴스화
        model = instantiate(cfg.model)
        
        # GPU 설정
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.descriptor_model.model = model.descriptor_model.model.to(device)
        model.descriptor_model.model.device = device
        
        if hasattr(model.segmentor_model, "predictor"):
            model.segmentor_model.predictor.model = model.segmentor_model.predictor.model.to(device)
        else:
            model.segmentor_model.model.setup_model(device=device, verbose=True)
        
        print(f"Model loaded successfully on {device}")
        return True
        
    except Exception as e:
        print(f"Error loading model: {e}")
        return False

async def load_templates():
    """템플릿 로딩 함수"""
    global templates_data, templates_masks, templates_boxes
    
    try:
        template_dir = os.getenv('ISM_TEMPLATE_DIR', '/workspace/Estimation_Server/SAM-6D/SAM-6D/Data/Example/outputs/templates')
        
        if not os.path.exists(template_dir):
            print(f"Template directory not found: {template_dir}")
            return False
        
        templates_data, templates_masks, templates_boxes = load_templates_from_files(template_dir, device)
        print(f"Templates loaded successfully: {len(templates_data)} templates")
        return True
        
    except Exception as e:
        print(f"Error loading templates: {e}")
        return False

async def load_cad_model():
    """CAD 모델 로딩 함수"""
    global cad_points
    
    try:
        cad_path = os.getenv('ISM_CAD_MODEL_PATH', '/workspace/Estimation_Server/SAM-6D/SAM-6D/Data/Example/obj_000005.ply')
        
        if not os.path.exists(cad_path):
            print(f"CAD model not found: {cad_path}")
            return False
        
        mesh = trimesh.load_mesh(cad_path)
        cad_points = mesh.sample(2048).astype(np.float32) / 1000.0
        print(f"CAD model loaded successfully: {cad_points.shape}")
        return True
        
    except Exception as e:
        print(f"Error loading CAD model: {e}")
        return False
```

#### 2.3 Startup 이벤트 추가
```python
@app.on_event("startup")
async def startup_event():
    """서버 시작 시 모든 모델 로딩"""
    print("Starting ISM Server...")
    
    # 모델 로딩
    model_loaded = await load_model()
    if not model_loaded:
        print("Failed to load model")
        return
    
    # 템플릿 로딩
    templates_loaded = await load_templates()
    if not templates_loaded:
        print("Failed to load templates")
        return
    
    # CAD 모델 로딩
    cad_loaded = await load_cad_model()
    if not cad_loaded:
        print("Failed to load CAD model")
        return
    
    print("All models loaded successfully!")
```

#### 2.4 모델 상태 확인 API 추가
```python
@app.get("/model/status")
async def model_status():
    """모델 로딩 상태 확인"""
    return {
        "model_loaded": model is not None,
        "templates_loaded": templates_data is not None,
        "cad_loaded": cad_points is not None,
        "device": str(device) if device else None,
        "num_templates": len(templates_data) if templates_data is not None else 0
    }
```

#### 2.5 Docker 컨테이너 내부에서 테스트
```bash
# 컨테이너 내부에서 서버 실행
cd /workspace/Estimation_Server/ISM_Server
uvicorn main:app --host 0.0.0.0 --port 8002

# 다른 터미널에서 컨테이너 접속하여 모델 상태 확인
docker exec -it sam6d-server bash
curl http://localhost:8002/model/status
```

#### 2.6 성공 기준
- ✅ 서버 시작 시 모델이 정상적으로 로딩됨
- ✅ 템플릿 데이터가 정상적으로 로딩됨
- ✅ CAD 모델이 정상적으로 로딩됨
- ✅ `/model/status` API가 정상적으로 응답함
- ✅ GPU 메모리 사용량이 적절함

---

### Phase 3: 추론 API 구현 및 테스트
**목표**: 실제 추론 기능을 수행하는 API 구현

#### 3.1 요청/응답 스키마 추가
```python
from pydantic import BaseModel
from typing import Optional

class InferenceRequest(BaseModel):
    rgb_image: str          # Base64 인코딩된 RGB 이미지
    depth_image: str        # Base64 인코딩된 깊이 이미지
    cam_params: dict        # 카메라 파라미터
    template_dir: Optional[str] = None  # 템플릿 디렉토리 경로 (선택사항)
    cad_path: Optional[str] = None      # CAD 모델 경로 (선택사항)

class InferenceResponse(BaseModel):
    success: bool
    detections: list
    inference_time: float
    template_dir_used: str
    cad_path_used: str
    error_message: Optional[str] = None
```

#### 3.2 이미지 처리 함수 추가
```python
def base64_to_image(base64_string):
    """Base64 문자열을 PIL Image로 변환"""
    try:
        image_data = base64.b64decode(base64_string)
        image = Image.open(io.BytesIO(image_data))
        return image
    except Exception as e:
        raise ValueError(f"Invalid base64 image: {e}")

def image_to_numpy(image):
    """PIL Image를 numpy array로 변환"""
    return np.array(image.convert("RGB"))
```

#### 3.3 추론 API 구현
```python
@app.post("/api/v1/inference", response_model=InferenceResponse)
async def inference(request: InferenceRequest):
    """추론 API"""
    start_time = time.time()
    
    try:
        # 입력 검증
        if not model or not templates_data or not cad_points:
            return InferenceResponse(
                success=False,
                detections=[],
                inference_time=0,
                template_dir_used="",
                cad_path_used="",
                error_message="Model not loaded"
            )
        
        # 이미지 변환
        rgb_image = base64_to_image(request.rgb_image)
        depth_image = base64_to_image(request.depth_image)
        
        rgb_array = image_to_numpy(rgb_image)
        depth_array = image_to_numpy(depth_image)
        
        # 카메라 파라미터 처리
        cam_params = request.cam_params
        depth_batch = batch_input_data_from_params(depth_array, cam_params, device)
        
        # 템플릿 경로 처리
        template_dir = request.template_dir or os.getenv('ISM_TEMPLATE_DIR')
        cad_path = request.cad_path or os.getenv('ISM_CAD_MODEL_PATH')
        
        # 추론 실행 (기존 함수 활용)
        result = run_inference_core(
            model=model,
            rgb_array=rgb_array,
            depth_batch=depth_batch,
            cad_points=cad_points,
            templates_data=templates_data,
            templates_masks=templates_masks,
            templates_boxes=templates_boxes,
            device=device,
            output_dir=None,  # 파일 저장 안함
            save_async=False
        )
        
        inference_time = time.time() - start_time
        
        return InferenceResponse(
            success=True,
            detections=result["detections"].to_dict() if hasattr(result["detections"], 'to_dict') else [],
            inference_time=inference_time,
            template_dir_used=template_dir,
            cad_path_used=cad_path,
            error_message=None
        )
        
    except Exception as e:
        inference_time = time.time() - start_time
        return InferenceResponse(
            success=False,
            detections=[],
            inference_time=inference_time,
            template_dir_used="",
            cad_path_used="",
            error_message=str(e)
        )
```

#### 3.4 테스트용 샘플 데이터 생성
```python
@app.get("/test/sample")
async def get_sample_data():
    """테스트용 샘플 데이터 반환"""
    # 기존 예제 데이터 사용
    rgb_path = "/workspace/Estimation_Server/SAM-6D/SAM-6D/Data/Example/rgb.png"
    depth_path = "/workspace/Estimation_Server/SAM-6D/SAM-6D/Data/Example/depth.png"
    cam_path = "/workspace/Estimation_Server/SAM-6D/SAM-6D/Data/Example/camera.json"
    
    try:
        # 이미지를 Base64로 변환
        with open(rgb_path, "rb") as f:
            rgb_base64 = base64.b64encode(f.read()).decode()
        
        with open(depth_path, "rb") as f:
            depth_base64 = base64.b64encode(f.read()).decode()
        
        # 카메라 파라미터 로딩
        import json
        with open(cam_path, "r") as f:
            cam_params = json.load(f)
        
        return {
            "rgb_image": rgb_base64,
            "depth_image": depth_base64,
            "cam_params": cam_params
        }
        
    except Exception as e:
        return {"error": str(e)}
```

#### 3.5 Docker 컨테이너 내부에서 테스트
```bash
# 컨테이너 내부에서 서버 실행
cd /workspace/Estimation_Server/ISM_Server
uvicorn main:app --host 0.0.0.0 --port 8002

# 다른 터미널에서 컨테이너 접속하여 테스트
docker exec -it sam6d-server bash

# 샘플 데이터 가져오기
curl http://localhost:8002/test/sample > sample_data.json

# 추론 테스트
curl -X POST "http://localhost:8002/api/v1/inference" \
  -H "Content-Type: application/json" \
  -d @sample_data.json
```

#### 3.6 성공 기준
- ✅ 샘플 데이터가 정상적으로 반환됨
- ✅ 추론 API가 정상적으로 응답함
- ✅ 추론 결과가 올바른 형식으로 반환됨
- ✅ 추론 시간이 적절함 (5초 이내)
- ✅ 에러 처리가 정상적으로 동작함

---

### Phase 4: Docker Compose 통합 및 최종 테스트
**목표**: Docker Compose를 통해 독립적인 ism-server 서비스로 배포

#### 4.1 Docker Compose 설정 추가
기존 `docker-compose.sam6d.yml`에 ism-server 서비스 추가 확인

#### 4.2 독립적인 ism-server 컨테이너 테스트
```bash
# ism-server 컨테이너 실행
docker-compose -f docker-compose.sam6d.yml up -d ism-server

# 로그 확인
docker-compose -f docker-compose.sam6d.yml logs -f ism-server

# 컨테이너 내부에서 테스트
docker exec -it ism-server bash
curl http://localhost:8002/health
curl http://localhost:8002/model/status
```

#### 4.3 외부에서 접근 테스트
```bash
# 호스트에서 접근 테스트
curl http://localhost:8002/health
curl http://localhost:8002/model/status

# 추론 테스트
curl -X POST "http://localhost:8002/api/v1/inference" \
  -H "Content-Type: application/json" \
  -d @sample_data.json
```

#### 4.4 성능 테스트
```bash
# 동시 요청 테스트
for i in {1..5}; do
  curl -X POST "http://localhost:8002/api/v1/inference" \
    -H "Content-Type: application/json" \
    -d @sample_data.json &
done
wait
```

#### 4.5 성공 기준
- ✅ ism-server 컨테이너가 정상적으로 시작됨
- ✅ 모든 API 엔드포인트가 정상적으로 동작함
- ✅ 모델 로딩이 정상적으로 완료됨
- ✅ 추론 기능이 정상적으로 동작함
- ✅ 외부에서 접근 가능함
- ✅ 동시 요청 처리가 가능함

---

## 버그 잡기 전략

### 1. 로그 기반 디버깅
- 각 단계마다 상세한 로그 출력
- 에러 발생 시 구체적인 에러 메시지 제공
- 성능 메트릭 수집 (로딩 시간, 추론 시간)

### 2. 단계별 검증
- 각 Phase 완료 후 반드시 테스트 실행
- 실패 시 이전 단계로 돌아가서 문제 해결
- 성공 기준을 만족할 때까지 다음 단계로 진행하지 않음

### 3. 점진적 복잡성 증가
- Phase 1: 가장 기본적인 기능부터
- Phase 2: 모델 로딩 (가장 복잡한 부분)
- Phase 3: 추론 기능 (비즈니스 로직)
- Phase 4: 배포 환경 (운영 환경)

### 4. 테스트 데이터 활용
- 기존 SAM-6D 예제 데이터 활용
- 실제 데이터로 테스트하여 현실적인 검증
- 다양한 입력 케이스 테스트

---

## 예상 문제점 및 해결 방안

### 1. 모델 로딩 실패
**문제**: GPU 메모리 부족, 모델 파일 경로 오류
**해결**: 메모리 사용량 모니터링, 경로 검증 로직 추가

### 2. 템플릿 로딩 실패
**문제**: 템플릿 파일 누락, 파일 형식 오류
**해결**: 파일 존재 여부 검증, 파일 형식 검증

### 3. 추론 성능 문제
**문제**: 추론 시간 과다, 메모리 누수
**해결**: 성능 프로파일링, 메모리 모니터링

### 4. Docker 환경 문제
**문제**: 볼륨 마운트 실패, 환경 변수 설정 오류
**해결**: 볼륨 경로 검증, 환경 변수 확인

이 계획을 따라 단계별로 구현하면 안정적이고 확장 가능한 ISM 서버를 구축할 수 있습니다.
