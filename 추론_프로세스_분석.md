# 추론 프로세스 전체 흐름

## 전체 아키텍처

```
Main_Server (워크플로우 오케스트레이션)
    ↓
[1] Render_Server (템플릿 생성)
    ↓ (생성된 템플릿)
[2] ISM_Server (객체 탐지 & 세그멘테이션)
    ↓ (탐지 결과)
[3] PEM_Server (포즈 추정)
    ↓ (최종 포즈 결과)
Output
```

---

## 1단계: 템플릿 생성 (Render_Server)

### 목적
3D CAD 모델을 다양한 관점에서 렌더링하여 템플릿 생성

### 입력
- `cad_path`: 3D CAD 모델 경로 (.ply 파일)
- `output_dir`: 템플릿 저장 디렉토리

### 처리 과정
```python
# Render_Server/render_custom_templates.py
blenderproc run render_custom_templates.py \
    --cad_path <CAD 파일> \
    --output_dir <출력 디렉토리>
```

1. **BlenderProc 환경 구성**
   - Blender 엔진 초기화
   - 카메라 포즈 로드 (42개 관점)
   
2. **3D 모델 로드**
   - `.ply` 파일 로드
   - 재질 설정 (옵션: 단색 컬러링)

3. **다중 관점 렌더링**
   - 각 카메라 포즈에서 렌더링:
     - **RGB 이미지** (`rgb_*.png`): 컬러 이미지
     - **Mask 이미지** (`mask_*.png`): 객체 영역 마스크
     - **Point Cloud** (`xyz_*.npy`): 3D 좌표

4. **출력**
   - 42개 템플릿 생성 (각 관점마다 3개 파일)
   - 총 126개 파일

### API
```http
POST http://localhost:8004/render/templates
{
    "cad_path": "/workspace/.../obj_000001.ply",
    "output_dir": "/workspace/.../templates/lm/obj_000001"
}
```

---

## 2단계: 객체 탐지 & 세그멘테이션 (ISM_Server)

### 목적
RGB/Depth 이미지에서 객체를 탐지하고 분할 (Instance Segmentation)

### 입력
- `rgb_image`: RGB 이미지 (base64 인코딩)
- `depth_image`: Depth 이미지 (base64 인코딩)
- `cam_params`: 카메라 파라미터 (intrinsics)
- `template_dir`: 생성된 템플릿 디렉토리
- `cad_path`: CAD 모델 경로
- `output_dir`: 결과 저장 디렉토리

### 처리 과정
```python
# ISM_Server/main.py - inference()
```

1. **이미지 디코딩**
   ```python
   rgb_array = base64_to_image(request.rgb_image)
   depth_array = depth_image_to_numpy(depth_image)
   ```

2. **템플릿 로드**
   - 템플릿 디렉토리에서 42개 템플릿 로드
   - `rgb_*.png`, `xyz_*.npy` 파일 읽기

3. **SAM 모델 추론**
   ```python
   # SAM(Scene-Product AModal) 모델 사용
   detections = model.inference(
       rgb_array,
       depth_array,
       cam_params,
       templates
   )
   ```

4. **탐지 결과 출력**
   - **bbox**: 바운딩 박스 좌표
   - **score**: 신뢰도 점수
   - **segmentation**: RLE 마스크
   - **object_id**: 객체 ID

### 출력
```json
{
    "success": true,
    "detections": {
        "boxes": [[x1, y1, x2, y2], ...],
        "scores": [0.95, 0.87, ...],
        "masks": [...],
        "object_ids": [1, 1, ...]
    },
    "inference_time": 1.234
}
```

### API
```http
POST http://localhost:8002/api/v1/inference
{
    "rgb_image": "<base64>",
    "depth_image": "<base64>",
    "cam_params": {"fx": 525.0, "fy": 525.0, ...},
    "template_dir": "/workspace/.../templates/lm/obj_000001",
    "cad_path": "/workspace/.../obj_000001.ply",
    "output_dir": "/workspace/.../output/ism"
}
```

---

## 3단계: 포즈 추정 (PEM_Server)

### 목적
탐지된 객체의 3D 위치와 자세를 정밀하게 추정

### 입력
- `rgb_image`: RGB 이미지
- `depth_image`: Depth 이미지
- `cam_params`: 카메라 파라미터
- `cad_path`: CAD 모델 경로
- `template_dir`: 템플릿 디렉토리
- `seg_data`: ISM 탐지 결과 (세그멘테이션 데이터)
- `det_score_thresh`: 신뢰도 임계값 (기본 0.2)

### 처리 과정
```python
# PEM_Server/api/endpoints/pose_estimation.py
```

1. **입력 준비**
   - Base64 이미지 디코딩
   - ISM 결과 로드 (`seg_data`)
   - 템플릿 로드

2. **후보군 필터링** (추가 기능)
   ```python
   # 상위 N개만 선택 (CUDA 메모리 절약)
   top_detections = sorted_detections[:5]
   ```

3. **Point Cloud 변환**
   ```python
   # Depth 이미지 → 3D Point Cloud
   points = depth_to_pointcloud(depth_array, cam_params)
   ```

4. **퓨전 추론** (6D Pose Estimation)
   ```python
   # SAM-6D 모델
   poses = model.estimate_pose(
       points,
       templates,
       detections
   )
   ```

5. **ICP 보정** (Optional)
   - Iterative Closest Point
   - 포즈 정밀도 향상

### 출력
```json
{
    "success": true,
    "poses": [
        {
            "translation": [x, y, z],
            "rotation": [qx, qy, qz, qw],
            "confidence": 0.92
        }
    ]
}
```

### API
```http
POST http://localhost:8003/api/v1/pose-estimation
{
    "rgb_image": "<base64>",
    "depth_image": "<base64>",
    "cam_params": {...},
    "cad_path": "...",
    "template_dir": "...",
    "seg_data": [{
        "bbox": [x1, y1, x2, y2],
        "score": 0.95,
        "segmentation": {...}
    }],
    "det_score_thresh": 0.2,
    "output_dir": "..."
}
```

---

## 전체 파이프라인 실행 (Main_Server)

### 워크플로우 오케스트레이션
```python
# Main_Server/services/workflow_service.py
async def execute_full_pipeline(
    class_name: str,
    object_name: str,
    input_images: Dict[str, str]
) -> Dict[str, Any]:
```

### 실행 순서

1. **템플릿 확인**
   - 템플릿 없으면 Render_Server 호출
   - 있으면 스킵

2. **ISM 추론**
   - RGB/Depth 이미지 + 템플릿 → 객체 탐지

3. **PEM 추론**
   - ISM 결과 + 템플릿 → 포즈 추정

4. **결과 통합**
   - 모든 결과를 하나의 디렉토리에 저장

### API
```http
POST http://localhost:8001/api/v1/workflow/full-pipeline
{
    "class_name": "lm",
    "object_name": "obj_000001",
    "input_images": {
        "rgb_path": "static/test/rgb.png",
        "depth_path": "static/test/depth.png",
        "camera_path": "static/test/camera.json"
    },
    "output_dir": "static/output/result_001"
}
```

---

## 데이터 흐름

```
[입력 이미지]
    RGB (720x1280x3)
    Depth (720x1280, float32)
    
    ↓ [ISM]
    
[탐지 결과]
    Bounding Box: [x1, y1, x2, y2]
    Score: 0.95
    Segmentation: RLE mask
    
    ↓ [PEM]
    
[최종 포즈]
    Translation: [0.123, 0.456, 0.789] (meters)
    Rotation: Quaternion [qx, qy, qz, qw]
    Confidence: 0.92
```

---

## 주요 특징

### 1. 템플릿 기반 매칭
- 미리 생성된 템플릿과 실시간 이미지 매칭
- 다양한 관점에서 렌더링된 템플릿 사용

### 2. 멀티 모달 퓨전
- **RGB**: 텍스처 정보
- **Depth**: 3D 거리 정보
- **Template**: 사전 렌더링된 객체

### 3. 2단계 추론
- **ISM**: "무엇인가?" (Detection)
- **PEM**: "어디에 어떻게?" (6D Pose)

### 4. 온라인 추론
- 실시간 이미지 처리
- Docker 컨테이너로 서비스화

---

## 성능 최적화

### 1. 템플릿 사전 생성
- 런타임 렌더링 비용 제거
- 42개 템플릿으로 충분한 커버리지

### 2. 후보군 제한
- ISM 결과 중 상위 N개만 PEM에 전달
- CUDA 메모리 절약 (기본 5개)

### 3. Async 처리
- 백그라운드 작업 지원
- Main_Server에서 비동기 오케스트레이션

---

## 에러 처리

### 각 단계별 에러 핸들링
- **Render**: 템플릿 생성 실패 → 건너뛰기
- **ISM**: 탐지 실패 → PEM 스킵
- **PEM**: 포즈 추정 실패 → 기존 탐지 결과 반환

### 모델 로딩 상태 체크
- 각 서버 헬스 체크
- 모델 미로딩 시 503 에러

